Experiments Log
===============

Last Modified Timestamp
-----------------------

Post-Edit Feedback - Iteration 1
--------------------------------

`This experiment <https://meta.wikimedia.org/wiki/Research:Edit_feedback>`_ measures the effect of delivering simple feedback to new users completing their first edits.

Data Cleaning an Consistency Verification
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


**Importing Logs:**

First the data is imported into tables on db42 via the click tracking logs from emery: ::

    	>>> el = DL.ExperimentsLoader()                                                                                                                                                                           
	>>> lpm = DL.ExperimentsLoader.LineParseMethods()
   	>>> flist = ["clicktracking.log-20120731.gz", "clicktracking.log-20120801.gz", "clicktracking.log-20120802.gz", "clicktracking.log-20120803.gz", "clicktracking.log-20120804.gz", "clicktracking.log-20120805.gz", "clicktracking.log-20120806.gz"]
    	>>> for f in flist:
        ...     el.create_table_from_xsv(f, '', 'e3_pef_iter1_log_data', parse_function=lpm.e3_pef_log_parse, regex_list=['ext.postEditFeedback'])
    	...
    	Aug-06 22:02:08 INFO     Inserting 2389 records into e3_pef_iter1_log_data
    	Aug-06 22:02:09 INFO     Inserting 8196 records into e3_pef_iter1_log_data
    	Aug-06 22:02:10 INFO     Inserting 8999 records into e3_pef_iter1_log_data
    	Aug-06 22:02:12 INFO     Inserting 9545 records into e3_pef_iter1_log_data
    	Aug-06 22:02:13 INFO     Inserting 9568 records into e3_pef_iter1_log_data
    	Aug-06 22:02:15 INFO     Inserting 8884 records into e3_pef_iter1_log_data
    	Aug-06 22:02:16 INFO     Inserting 9819 records into e3_pef_iter1_log_data
	
	Aug-15 20:11:06 INFO     Inserting 7973 records into e3_pef_iter1_log_data
	Aug-15 20:11:08 INFO     Inserting 2687 records into e3_pef_iter1_log_data
	Aug-15 20:11:09 INFO     Inserting 1502 records into e3_pef_iter1_log_data
	Aug-15 20:11:11 INFO     Inserting 1019 records into e3_pef_iter1_log_data
	Aug-15 20:11:13 INFO     Inserting 857 records into e3_pef_iter1_log_data
	Aug-15 20:11:14 INFO     Inserting 534 records into e3_pef_iter1_log_data
	Aug-15 20:11:16 INFO     Inserting 441 records into e3_pef_iter1_log_data


**User IDs derived from Click-tracking Revision IDs:**

After this was completed I associated rev ids with user ids from the revision table to get user "hashes" in the log date: ::

	>>> import classes.DataLoader as DL
	>>> dl = DL.e3_pef_iter1_log_data_Loader()
	>>> el = DL.ExperimentsLoader()
	>>> results = dl.execute_SQL('select rev_id, rev_user from rfaulk.e3_pef_iter1_log_data join enwiki.revision on rev_id = e3pef_rev_id where e3pef_event regexp "1-postEdit"')
	>>> rev_ids = el.get_elem_from_nested_list(results, 0)
	>>> user_ids = el.get_elem_from_nested_list(results, 1)
	>>> dl.update_row('e3pef_user_hash', user_ids, 'e3pef_rev_id', rev_ids)
	Aug-06 15:31:09 INFO     32677 rows successfully updated in rfaulk.e3_pef_iter1_log_data


**Verifying User-IDs:**

Let's begin by looking at the count of total post-edit events contrasted with the number of post-edit events generated by a user with a registration data falling within the eligibility window of the experiment ::

	mysql> select count(*) from rfaulk.e3_pef_iter1_log_data join user on user_id = e3pef_user_hash where e3pef_event regexp '1-postEdit';

	+----------+
	| count(*) |
	+----------+
	|    16457 |
	+----------+

	mysql> select count(*) from rfaulk.e3_pef_iter1_log_data join user on user_id = e3pef_user_hash where e3pef_event regexp '1-postEdit' and user_registration < '20120730000000';

	+----------+
	| count(*) |
	+----------+
	|      331 |
	+----------+


We see that there are a relatively small number of user_ids falling outside of the valid registration period.  Since this is a relatively small value we can safely filter these users out for the moment, however we will want to revisit the potential causes here. ::

	mysql> select count(distinct e3pef_user_hash) unique_users from rfaulk.e3_pef_iter1_log_data join user on user_id = e3pef_user_hash where e3pef_event regexp '1-postEdit' and user_registration > '20120730000000'; 

	+--------------+
	| unique_users |
	+--------------+
	|         4585 |
	+--------------+

	mysql> select count(*) as users_duplicate from (select e3pef_user_hash, count(distinct e3pef_user_hash, e3pef_event) as events from rfaulk.e3_pef_iter1_log_data join user on user_id = e3pef_user_hash where e3pef_event regexp '1-postEdit' and user_registration > '20120730000000' group by 1) as t1 where events > 1;

	+-----------------+
	| users_duplicate |
	+-----------------+
	|              19 |
	+-----------------+

	mysql> select e3pef_user_hash from (select e3pef_user_hash, count(distinct e3pef_user_hash, e3pef_event) as events from rfaulk.e3_pef_iter1_log_data join user on user_id = e3pef_user_hash where e3pef_event regexp '1-postEdit' and user_registration > '20120730000000' group by 1) as t1 where events > 1;
	
	+-----------------+
	| e3pef_user_hash |
	+-----------------+
	| 17244391        |
	| 17249448        |
	| 17251054        |
	| 17251257        |
	| 17253687        |
	| 17253791        |
	| 17256491        |
	| 17257303        |
	| 17257370        |
	| 17260056        |
	| 17262157        |
	| 17264093        |
	| 17264897        |
	| 17266322        |
	| 17268608        |
	| 17269241        |
	| 17269801        |
	| 17270991        |
	| 17275055        |
	+-----------------+

The first query tells us how many unique users were issued feedback.  Next we want to ensure that users are not seeing multiple events - the second query tells us that only 19 users are.  This will be another area to investigate, but for now we can safely filter the IDs for the time being.


**Verifying Page IDs:**

Next we will verify that the pages paired with the rev_ids match by simply seeing if any exist: ::

	mysql> select count(*) as page_mismatches from (select * from rfaulk.e3_pef_iter1_log_data join enwiki.revision on rev_id = e3pef_rev_id where e3pef_page_id != rev_page) as t;

	+-----------------+
	| page_mismatches |
	+-----------------+
	|              74 |
	+-----------------+

	+---------------+
	| total_records |
	+---------------+
	|         55850 |
	+---------------+

The number of mismatched page_ids is negligible next to the total number of records.  While we should try to determine what the issue is here, for the moment we can safely filter these records out of the sample to proceed.


**Ensuring that user tokens are unique:**

This check will allow us to determine further the validity of the generated user IDs.  The underlying assumption is that the revision from the log data will reliably report the user that received the feedback for that revision.  I followed up with a couple queries to count the number of user tokens for which there are multiple IDs: ::


	mysql> select count(*) from (select e3pef_user_token, count(distinct e3pef_user_token, e3pef_user_hash) as tokens from rfaulk.e3_pef_iter1_log_data join enwiki.user on user_id = cast(e3pef_user_hash as UNSIGNED) where user_registration > '20120730000000' and e3pef_event regexp 
	
	+----------+
	| count(*) |
	+----------+
	|     6925 |
	+----------+
	
	
	mysql> select count(*) from (select e3pef_user_token, count(distinct e3pef_user_token, e3pef_user_hash) as tokens from rfaulk.e3_pef_iter1_log_data join enwiki.user on user_id = cast(e3pef_user_hash as UNSIGNED) where user_registration > '20120730000000' and e3pef_event regexp '1-postEdit' group by 1) as t where t.tokens > 1;
	
	+----------+
	| count(*) |
	+----------+
	|       82 |
	+----------+

The number is small, but potentially concerning since it simply shouldn't be possible.  To workaround this small subset of inconsistent tokens we will remove users that non-uniquely map to user tokens.  This should leave the bulk of our data set in tact.


**Building New Metrics:**


First the table needs to be altered to add columns that store the new metrics: ::

	mysql> alter table rfaulk.e3_pef_iter1_log_data add column e3pef_time_to_milestone varbinary(255)
	mysql> alter table rfaulk.e3_pef_iter1_log_data add column e3pef_revision_measure varbinary(255)

As a first milestone we will examine the time between the first and the second edit. 

#. Getting all users from the dataset that made more than one edit and registered in the experimental period.
#. Computing for each user the time, in minutes, between the first and second edit
#. Updating the PEF-1 table with these values

::

	>>> import classes.Metrics as m
	>>> import classes.DataLoader as DL
	>>> threshold_metric = m.TimeToThreshold(m.TimeToThreshold.EDIT_COUNT_THRESHOLD, first_edit=1, threshold_edit=2)
	>>> sql = "select e3pef_user_hash, e3pef_event from rfaulk.e3_pef_iter1_log_data join enwiki.user on e3pef_user_hash = user_id where e3pef_event regexp '1-postEdit' and user_registration > '20120730000000' group by 1,2 having count(*) > 1"
	>>> users = threshold_metric._datasource_.execute_SQL(sql)
	>>> users = dl.get_elem_from_nested_list(users,0)
	>>> thresholds = threshold_metric.process(users)
	>>> mins_to_threshold = dl.get_elem_from_nested_list(thresholds,1)
	>>> tl = DL.e3_pef_iter1_log_data_Loader()
	>>> tl.update_row('e3pef_time_to_milestone', mins_to_threshold, 'e3pef_user_hash', users)
	Aug-10 01:30:15 INFO     2252 rows successfully updated in rfaulk.e3_pef_iter1_log_data

Here there are a total of 2252 users that went on to make more than one edit.  The last metric that we will compute before is user survival.  However, this metric is best left to compute retro-actively at a future date sine the effects of survival may still be in play and will still likely be observed after a period of time beyond the conclusion of the experiment.

Now that we have all of our metrics loaded and data verified we may move on to the experimental analysis.


**Importing Centralauth Local and Global User Tables:**

In order to ensure that accounts generated are on enwiki we need to reference the global and local user tables.  The centralauth database is accessible via fenari: ::

	rfaulk@fenari:~$ sql centralauth --batch -e 'select * from globaluser where gu_registration > "20120730000000"' > globaluser_082312.tsv
	rfaulk@fenari:~$ sql centralauth --batch -e 'select * from localuser where lu_attached_timestamp > "20120730000000"' > localuser_082312.tsv

Next we import these records into rfaulk.globaluser and rfaulk.localuser on db42: ::

	>>> dl.create_table_from_xsv('globaluser_082312.tsv','','globaluser')
	Aug-23 13:12:10 INFO     Inserting 204602 records into globaluser
	>>> dl.create_table_from_xsv('localuser_082312.tsv','','localuser')
	Aug-23 13:12:53 INFO     Inserting 646710 records into localuser


Analysis
^^^^^^^^

We determined that several filters needed to be applied to clean the dataset over which analysis would be applied. The details can be found `here`_.  Below is the processing performed in the python interpreter environment to build the tables for edit count, time to threshold, and bytes added metrics. ::

.. _here: https://meta.wikimedia.org/wiki/Research:Post-edit_feedback#PEF-1:_Confirmation_vs._Gratitude

	>>> import classes.DataLoader as DL
	>>> import Metrics as M
	>>> more_excluded_users = dl.get_elem_from_nested_list(dl.execute_SQL('SELECT DISTINCT user_id FROM dartar.e3_pef_iter1_global WHERE gudiff < -7;'),0)
	>>> more_excluded_users = dl.cast_elems_to_string(more_excluded_users)
	>>> exclude_users = dl.get_elem_from_nested_list(dl.execute_SQL('select distinct ns.user_id from halfak.pef1_blocked as b join rfaulk.e3_pef_iter1_ns as ns on ns.user_id = b.user_id;'),0)
	>>> exclude_users.extend(more_excluded_users)
	>>> exclude_users_str = dl.format_comma_separated_list(exclude_users)

	>>> sql = 'select distinct user_id from rfaulk.e3_pef_iter1_ns where not(user_id in (%s))' % exclude_users_str
	>>> eligible_users = dl.get_elem_from_nested_list(dl.execute_SQL(sql),0)

	>>> start_date = datetime.datetime(year=2012,month=7,day=30)
	.. end_date = datetime.datetime(year=2012,month=8,day=14)
	
	>>> ec = M.EditCount(date_start=start_date, date_end=end_date, raw_count=False).process(eligible_users)
	>>> o = list()
	>>> for i in ec.keys(): 
	>>> 	e = [i] 
	>>> 	e.extend(ec[i]) 
	>>> 	o.append(e)
	>>> dl.list_to_xsv(o)
	>>> dl.create_table_from_xsv('list_to_xsv.out', el.E3_PEF_EC_TABLE, 'e3_pef_iter1_editcount', create_table=True)

	>>> ttt = M.TimeToThreshold(M.TimeToThreshold.EDIT_COUNT_THRESHOLD, first_edit=1, threshold_edit=2).process(eligible_users)
	>>> dl.list_to_xsv(ttt)
	>>> dl.create_table_from_xsv('list_to_xsv.out', el.E3_PEF_TTT_TABLE, 'e3_pef_iter1_timetothreshold', create_table=True)
	>>> dl.create_xsv_from_SQL('select r.user_id, d.bucket, r.time_minutes from rfaulk.e3_pef_iter1_timetothreshold as r join dartar.e3_pef_iter1_users as d on d.user_id = r.user_id;', outfile = 'e3_pef_iter1_ttt_bucket.tsv')

	>>> bytes_added = M.BytesAdded(date_start='2012-07-30 00:00:00', raw_count=False, mode=1).process(eligible_users)
	>>> o = list()
	>>> for i in bytes_added.keys(): 
	>>> 	e = [i] 
	>>> 	e.extend(bytes_added[i]) 
	>>> 	o.append(e)
	>>> dl.list_to_xsv(o)
	>>> dl.create_table_from_xsv('list_to_xsv.out', el.E3_PEF_BA_TABLE, 'e3_pef_iter1_bytesadded', create_table=True)
	>>> dl.create_xsv_from_SQL('select r.user_id, d.bucket, r.bytes_added_net, r.bytes_added_abs, r.bytes_added_pos, r.bytes_added_neg, r.edit_count from rfaulk.e3_pef_iter1_bytesadded as r join dartar.e3_pef_iter1_users as d on d.user_id = r.user_id;')

Once the data was written to .tsv files it was imported into R for "edit_count", "bytes added", and "time to threshold": ::

	> pef_data = read.table("/Users/rfaulkner/projects/data/e3_pef_iter1_editcount_bucket.tsv", na.strings="\\N", sep="\t", comment.char="", quote="", header=T)
	> pef_data_ttt = read.table("/Users/rfaulkner/projects/data/e3_pef_iter1_ttt_bucket.tsv", na.strings="\\N", sep="\t", comment.char="", quote="", header=T)
	> pef_data_ba = read.table("/Users/rfaulkner/projects/data/e3_pef_iter1_ba_bucket.tsv", na.strings="\\N", sep="\t", comment.char="", quote="", header=T)

Each of these datasets include a "bucket" field to identify the treatment.  So next, the data was split by treatment for each metric: ::

	> pef_data_control <- pef_data[filter.list.by.regex("control", pef_data$bucket),]
	> pef_data_exp1 <- pef_data[filter.list.by.regex("experimental_1", pef_data$bucket),]
	> …
	> pef_data_ttt_exp1 <- pef_data_ttt[filter.list.by.regex("experimental_1", pef_data_ttt$bucket),]
	> …

For the each metric we tested whether the data was normal or log-normal using the Shapiro-Wilk normality test.  Below is an example - it was verified that the bytes added data for positive and absolute contributions is indeed log normal, and that time to threshold and edit count data was distributed normally with p-values indicating high significance.  The excerpt below shows how normality was tested on the "bytes added" data. ::

	> pef_data_log_treatment = log(pef_data_log_treatment$bytes_added_type[pef_data_log_treatment$bytes_added_type > 0])
	> shapiro.test(pef_data_ba_abs_log_exp2)

		Shapiro-Wilk normality test

		data:  pef_data_ba_abs_log_exp2 
		W = 0.9964, p-value = 4.054e-06

At this point it was suitable to use t-test on the data set to compare the control with the treatment groups: ::

	> t.test(x= pef_data_ba_type_log_control, y= pef_data_ba_type_log_treatment, alternative = "two.sided", paired = FALSE, var.equal = FALSE, conf.level = 0.95)

See the `results`_ containing further details on the analysis and the conclusions of the experiment.  Also, we have prepared a `blog post`_ summarizing the conclusions over the entire study.

.. _results: https://meta.wikimedia.org/wiki/Research:Post-edit_feedback/PEF-1
.. _blog post: https://blog.wikimedia.org/2012/09/24/giving-new-wikipedians-feedback-post-edit/


Post-Edit Feedback: Iteration 2
-------------------------------
